{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/Employee-Performance-Prediction/notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/Employee-Performance-Prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir:Path\n",
    "    data_path: Path\n",
    "    seq_length: int = 2\n",
    "    test_size: float = 0.25\n",
    "    random_state: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import read_yaml, create_directories, save_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config=self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "        data_tranformation_config=DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path\n",
    "        )\n",
    "        return data_tranformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from src import logger\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.encoders = {}\n",
    "        create_directories([self.config.root_dir])\n",
    "\n",
    "    def clean_and_encode_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = df.copy()\n",
    "\n",
    "            # Convert numeric columns to appropriate types\n",
    "            numeric_cols = [\n",
    "                'Age', 'DistanceFromHome', 'EmpEducationLevel', \n",
    "                'EmpEnvironmentSatisfaction', 'EmpHourlyRate', \n",
    "                'EmpJobInvolvement', 'EmpJobLevel', 'EmpJobSatisfaction',\n",
    "                'NumCompaniesWorked', 'EmpLastSalaryHikePercent', \n",
    "                'EmpRelationshipSatisfaction', 'TotalWorkExperienceInYears', \n",
    "                'TrainingTimesLastYear', 'EmpWorkLifeBalance', \n",
    "                'ExperienceYearsAtThisCompany', 'ExperienceYearsInCurrentRole',\n",
    "                'YearsSinceLastPromotion', 'YearsWithCurrManager', \n",
    "                'PerformanceRating'\n",
    "            ]\n",
    "            for col in numeric_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "            # Encode categorical columns\n",
    "            categorical_cols = ['Gender', 'EducationBackground', 'MaritalStatus', \n",
    "                                 'EmpDepartment', 'EmpJobRole', 'BusinessTravelFrequency', \n",
    "                                 'OverTime', 'Attrition']\n",
    "\n",
    "            for col in categorical_cols:\n",
    "                label_encoder = LabelEncoder()\n",
    "                df[col] = label_encoder.fit_transform(df[col])\n",
    "                self.encoders[f'{col}_encoder'] = label_encoder\n",
    "\n",
    "            # Fill missing numeric values with median\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "            logger.info(\"Data cleaning and encoding completed successfully\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in clean_and_encode_data: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def create_sequences(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.config.seq_length):\n",
    "            sequence = data[i:i + self.config.seq_length + 1]\n",
    "            if not np.any(np.isnan(sequence)):\n",
    "                X.append(sequence[:-1])\n",
    "                y.append(sequence[-1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def save_data(self, X: np.ndarray, y: np.ndarray, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Save the transformed data into a CSV file.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Feature data.\n",
    "            y (np.ndarray): Target data.\n",
    "            filename (str): Name of the file to save the data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Combined DataFrame of features and target.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reshape feature data for saving as 2D DataFrame\n",
    "            X_2d = X.reshape(X.shape[0], -1)\n",
    "            feature_cols = [f'time_step_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "            # Create DataFrame for features and target\n",
    "            X_df = pd.DataFrame(X_2d, columns=feature_cols)\n",
    "            y_df = pd.DataFrame(y, columns=['target'])\n",
    "\n",
    "            combined_df = pd.concat([X_df, y_df], axis=1)\n",
    "\n",
    "            # Save to file\n",
    "            save_path = Path(self.config.root_dir) / filename\n",
    "            combined_df.to_csv(save_path, index=False)\n",
    "\n",
    "            logger.info(f\"Data saved to: {save_path}\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def train_test_spliting(self):\n",
    "        try:\n",
    "            logger.info(\"Started data transformation\")\n",
    "\n",
    "            # Read the data from CSV path defined in config\n",
    "            data = pd.read_csv(self.config.data_path)\n",
    "            logger.info(f\"Read data from {self.config.data_path}, shape: {data.shape}\")\n",
    "\n",
    "            # Clean and encode the data\n",
    "            processed_df = self.clean_and_encode_data(data)\n",
    "            target_data = processed_df['PerformanceRating'].values\n",
    "\n",
    "            # Create sequences for the target data\n",
    "            X, y = self.create_sequences(target_data)\n",
    "\n",
    "            if len(X) == 0:\n",
    "                raise ValueError(\"No valid sequences could be created. Check data integrity.\")\n",
    "\n",
    "            logger.info(f\"Created sequences with shape X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, \n",
    "                test_size=self.config.test_size,\n",
    "                random_state=self.config.random_state\n",
    "            )\n",
    "\n",
    "            # Save train and test data to CSV files\n",
    "            train_df = self.save_data(X_train, y_train, \"train.csv\")\n",
    "            test_df = self.save_data(X_test, y_test, \"test.csv\")\n",
    "\n",
    "            # Save encoders using joblib\n",
    "            encoder_path = Path(self.config.root_dir) / \"encoders.joblib\"\n",
    "            joblib.dump(self.encoders, encoder_path)\n",
    "\n",
    "            logger.info(\"Data transformation completed\")\n",
    "            logger.info(f\"Training set shape: {train_df.shape}\")\n",
    "            logger.info(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "            return train_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in data transformation: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-17 13:16:38,792: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-01-17 13:16:38,794: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-01-17 13:16:38,798: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-01-17 13:16:38,798: INFO: common: created directory at: artifacts]\n",
      "[2025-01-17 13:16:38,799: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-01-17 13:16:38,800: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-01-17 13:16:38,801: INFO: 650467671: Started data transformation]\n",
      "[2025-01-17 13:16:38,806: INFO: 650467671: Read data from artifacts/data_ingestion/EPP-main/INX_Future_Inc_Employee_Performance_CDS_Project2_Data_V1.8.csv, shape: (1200, 28)]\n",
      "[2025-01-17 13:16:38,820: INFO: 650467671: Data cleaning and encoding completed successfully]\n",
      "[2025-01-17 13:16:38,825: INFO: 650467671: Created sequences with shape X: (1198, 2), y: (1198,)]\n",
      "[2025-01-17 13:16:38,829: INFO: 650467671: Data saved to: artifacts/data_transformation/train.csv]\n",
      "[2025-01-17 13:16:38,830: INFO: 650467671: Data saved to: artifacts/data_transformation/test.csv]\n",
      "[2025-01-17 13:16:38,832: INFO: 650467671: Data transformation completed]\n",
      "[2025-01-17 13:16:38,833: INFO: 650467671: Training set shape: (898, 3)]\n",
      "[2025-01-17 13:16:38,833: INFO: 650467671: Test set shape: (300, 3)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    transformer = DataTransformation(config=data_transformation_config)\n",
    "    train_df, test_df = transformer.train_test_spliting()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in data transformation: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
